<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-09-17T20:56:16+08:00</updated><id>http://localhost:4000/</id><title type="html">luo_13的博客</title><subtitle>别看了，没啥好看的。</subtitle><author><name>luo_13</name></author><entry><title type="html">简介-模板与重载</title><link href="http://localhost:4000/c++%20language/%E6%A8%A1%E6%9D%BF%E4%B8%8E%E9%87%8D%E8%BD%BD/" rel="alternate" type="text/html" title="简介-模板与重载" /><published>2018-09-15T00:00:00+08:00</published><updated>2018-09-15T00:00:00+08:00</updated><id>http://localhost:4000/c++%20language/%E6%A8%A1%E6%9D%BF%E4%B8%8E%E9%87%8D%E8%BD%BD</id><content type="html" xml:base="http://localhost:4000/c++%20language/%E6%A8%A1%E6%9D%BF%E4%B8%8E%E9%87%8D%E8%BD%BD/">&lt;h1 id=&quot;c模板函数与重载函数&quot;&gt;C++模板函数与重载函数&lt;/h1&gt;

&lt;h2 id=&quot;模板函数&quot;&gt;模板函数&lt;/h2&gt;

&lt;p&gt;可以理解为一段通用代码&lt;br /&gt;
形参类型由编译器决定&lt;br /&gt;
下述类型要相同，不同编译器会报错&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;更通用的版本&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Ta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Ta&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Ta&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tb&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tc&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;重载函数&quot;&gt;重载函数&lt;/h2&gt;

&lt;p&gt;定义多个同名函数的机制称为函数重载&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;
&lt;p&gt;模板函数和重载函数有什么不同&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;模板函数只能定义一个&lt;/li&gt;
  &lt;li&gt;重载函数可以多个&lt;/li&gt;
  &lt;li&gt;模板函数参数数量固定，而重载函数可以不定&lt;/li&gt;
&lt;/ul&gt;</content><author><name>luo_13</name></author><category term="C++ language" /><summary type="html">模板函数与重载函数</summary></entry><entry><title type="html">简介-RNN</title><link href="http://localhost:4000/ml/%E7%AE%80%E4%BB%8B-RNN/" rel="alternate" type="text/html" title="简介-RNN" /><published>2018-09-15T00:00:00+08:00</published><updated>2018-09-15T00:00:00+08:00</updated><id>http://localhost:4000/ml/%E7%AE%80%E4%BB%8B-RNN</id><content type="html" xml:base="http://localhost:4000/ml/%E7%AE%80%E4%BB%8B-RNN/">&lt;h1 id=&quot;vanilla-rnn&quot;&gt;vanilla RNN&lt;/h1&gt;

&lt;hr /&gt;

&lt;p&gt;RNN最大的特点在于同一隐藏层的各单元按照序列链接。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/RNN/1.png&quot; alt=&quot;RNN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;通过这个图可以知道，RNN比CNN更适合处理具有一定序列的输入，因为RNN的结构决定了前面的输入必然能影响到后面的输出。这使得RNN在NLP这一领域获得了很大的成功。&lt;/p&gt;

&lt;p&gt;在学习RNN的过程过程中反向传播公式的推导应该是比较难的一个部分，因为像是CNN的反向传播只有一个方向，就是上一层传递给下一层，而RNN除了沿上下层的传播还有沿时间序列方向的传播。&lt;/p&gt;

&lt;p&gt;最终推导结果：&lt;br /&gt;
c为隐藏层到输出的bias，b为输入到隐藏层的bais，推导过程比较长，以下这篇文章写得比较清晰&lt;a href=&quot;https://zybuluo.com/hanbingtao/note/541458&quot;&gt;循环神经网络&lt;/a&gt;以及&lt;a href=&quot;http://www.deeplearningbook.org/contents/rnn.html&quot;&gt;deeplearningbook&lt;/a&gt;写得比较清晰&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/RNN/2.png&quot; alt=&quot;RNN gradient&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在编程过程中发现了一个比较容易出错的地方，那就是反向传播过程要注意残差在上下层之间传递，因为参考内容的推导过程主要是以一个隐藏层作为例子，所以写代码的过程要注意这一点，如下图，残差3要往4和5的方向传递，但要注意的是，残差3是有1,2两个方向的残差累加而成的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/RNN/3.png&quot; alt=&quot;RNN flow&quot; /&gt;&lt;/p&gt;

&lt;p&gt;另一个需要注意的地方是，在某些任务（不是全部）中RNN的训练和测试是很不一样的，这里举的例子是cs231n的image captioning 问题。&lt;/p&gt;

&lt;p&gt;训练时因为知道正确的label，所以可以将ground-truth直接当成input输入到RNN当中，但是训练的时候，因为不知道ground-truth，所以要将前一个输出要当成后一个的输入。&lt;br /&gt;
拿上面的图为例就是用x(t – 1)预测出o(t-1),然后将o(t-1)当成x(t)再次输入到网络。&lt;/p&gt;

&lt;p&gt;采用这种方式是因为cs231n的assignment3将图像的特征当成input输入到网络时只会预测出结果中的第一个词。&lt;br /&gt;
采用的结构感觉是在one to many上做了一些变动。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/RNN/4.png&quot; alt=&quot;RNN flow&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;vanilla-rnn的缺点&quot;&gt;Vanilla RNN的缺点&lt;/h2&gt;
&lt;p&gt;训练过程中容易发生梯度爆炸或梯度消失，产生的原因在参考资料&lt;a href=&quot;https://zybuluo.com/hanbingtao/note/541458&quot;&gt;循环神经网络&lt;/a&gt;中有详细说明。&lt;/p&gt;

&lt;p&gt;梯度爆炸可以简单用阈值方式解决。&lt;br /&gt;
而解决梯度消失也有挺多的方式，参考资料&lt;a href=&quot;https://zybuluo.com/hanbingtao/note/541458&quot;&gt;循环神经网络&lt;/a&gt;中提到了三种常见的方式，我觉得进行batch normalization也应该可以缓解这种现象。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/RNN/5.png&quot; alt=&quot;RNN flow&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;lstm&quot;&gt;LSTM&lt;/h1&gt;

&lt;hr /&gt;

&lt;p&gt;如sigmoid函数，如果输入的模过大就会出现梯度消失的情况，LSTM为了解决这一问题加入了门和单元状态的概念，可以有效的控制前面输入对后面输出的影响。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/RNN/6.png&quot; alt=&quot;RNN flow&quot; /&gt;&lt;/p&gt;

&lt;p&gt;LSTM添加了三个门，遗忘门（forget gate）用以决定上一时刻的单元状态有多少能保留下来，输入门（input gate）用来决定输入状态有多少能进入到单元状态，输出门（output gate）用来决定单元状态有多少能输出到output&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/RNN/7.png&quot; alt=&quot;RNN flow&quot; /&gt;&lt;/p&gt;

&lt;p&gt;c为单元状态（相当于一个容器），从左往右的三个X分别为遗忘门，输入门和输出门。&lt;br /&gt;
反向传播公式的推导在参考资料&lt;a href=&quot;http://www.deeplearningbook.org/contents/rnn.html&quot;&gt;deeplearningbook&lt;/a&gt;和&lt;a href=&quot;https://zybuluo.com/hanbingtao/note/581764&quot;&gt;LSTM&lt;/a&gt;中有很详细的过程&lt;/p&gt;

&lt;figure class=&quot;third&quot;&gt;
    &lt;img src=&quot;/images/RNN/8-1.png&quot; /&gt;
    &lt;img src=&quot;/images/RNN/8-2.png&quot; /&gt;
    &lt;img src=&quot;/images/RNN/8-3.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;状态单元c并不直接输出，在反向传播计算残差的时候要格外注意这一点。&lt;/p&gt;

&lt;p&gt;LSTM还有很多不同的变体，比如说GRU，GRU网上资料说是LSTM最为成功的一种变体，在后面的学习中会详细的去了解。&lt;/p&gt;

&lt;p&gt;一些想法&lt;br /&gt;
RNN比较适合处理时序问题，之前还看过一篇博客，内容主要是在介绍RNN能做一些什么事情，里面比较有趣的是让电脑学会从左往右去看一张图片。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/RNN/9.gif&quot; alt=&quot;RNN flow&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;ocr&quot;&gt;OCR&lt;/h1&gt;

&lt;hr /&gt;

&lt;p&gt;对于text recognition有两种比较常见的方法&lt;br /&gt;
非端到端的OCR：需要进行预分割，特征提取，语义分析等环节，各环节之间存在一定关系&lt;br /&gt;
端到端OCR：无需进行预分割，且整个过程是一个整体&lt;/p&gt;</content><author><name>luo_13</name></author><category term="ML" /><summary type="html">主要介绍vanilla RNN与LSTM</summary></entry><entry><title type="html">简介-CTC</title><link href="http://localhost:4000/ml/%E7%AE%80%E4%BB%8B-CTC/" rel="alternate" type="text/html" title="简介-CTC" /><published>2018-09-15T00:00:00+08:00</published><updated>2018-09-15T00:00:00+08:00</updated><id>http://localhost:4000/ml/%E7%AE%80%E4%BB%8B-CTC</id><content type="html" xml:base="http://localhost:4000/ml/%E7%AE%80%E4%BB%8B-CTC/">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;h1 id=&quot;ctc&quot;&gt;CTC&lt;/h1&gt;

&lt;p&gt;CTC (Connectionist Temporal Classification) 是针对时间分类任务所设计出来的，利用它可以实现端到端的文本识别，在一定程度上解决了文本识别中的分割问题。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;通过上面的图片可以对CTC的工作有一个比较直观的理解。它不需要输入输出在每一个时刻都对齐，只要求最后的输出跟目标输出一致。&lt;/p&gt;

&lt;p&gt;CTC与别的算法主要区别在于，他要求输出序列不短于目标序列，且引入了Blank状态。&lt;/p&gt;

&lt;p&gt;如：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(a-ab-)=F(-aa--abb)=aab&lt;/script&gt;

&lt;p&gt;CTC主要在做的东西是预测每一个时刻的输出，然后合并相邻的重复输出，再移去blank状态。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这里，两个L之间blank状态有着很关键的作用，它可以分隔开来两个相同的输出，避免了得到helo的结果。&lt;/p&gt;

&lt;p&gt;从上图可以看出，CTC对输出序列有两个要求，第一个是输入序列要不短于label长度;第二个是为了能区分开两个相同字母相邻的情况，需要对label做一下处理，通常的操作是在label中每个元素的前后加上blank状态，这样做会使label长度由U变成2U+1。&lt;/p&gt;

&lt;p&gt;如:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z=[ε,y1,εy2,...,ε,yu,ε]&lt;/script&gt;

&lt;h1 id=&quot;loss-function&quot;&gt;LOSS function&lt;/h1&gt;

&lt;p&gt;计算前向通道的时候，我们可以采用合并相同路线的方式去简化计算，也就是说每一个节点的概率只计算一次，这样可以减少很多计算量。这类似于HMM的forward-backword Algorithm。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;计算前向通道的时候有一个值得注意的地方，那就是节点链接的合理性&lt;br /&gt;
比如说label集合是[ε1 a ε2 p ε3 p ε4 l ε5 e ε6],ε是blank，为了方便区分加了下  标，假设一个输出序列是正确的话&lt;br /&gt;
如果t时刻预测结果是e，则t-1的预测则一定是[l ε5 e]之间的一个;&lt;br /&gt;
如果 t时刻预测结果是ε6，则t-1的预测则一定是[e ε6]之间的一个;&lt;br /&gt;
如果 t时刻预测结果是p，则t-1的预测则一定是[ε3 p]之间的一个;&lt;br /&gt;
且一条正确的路径只能由[ε1 a]开始，以[e ε6]结束。&lt;/p&gt;

&lt;p&gt;所以前后节点的链接方式如下&lt;/p&gt;

&lt;center class=&quot;half&quot;&gt;
    &lt;img src=&quot;/images/CTC/4.png&quot; /&gt;  &lt;img src=&quot;/images/CTC/5.png&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;下图是一个有效路径的示意图&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/6.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;所以前后向算法在 &lt;br /&gt;
前向推导：&lt;br /&gt;
(1)初始化:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/7.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(2)递推关系：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/8.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/9.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;因为时间关系，有些路径不能到达终点 U‘-u-1 &amp;gt; 2(T-t)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/10.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;同理后向推导：&lt;br /&gt;
(1)初始化:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/11.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(2)递推关系：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/12.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/13.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CTC loss function使用的是最大似然&lt;/p&gt;

&lt;p&gt;取log之后&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/14.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;由前后向算法可得&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/15.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;所以&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/16.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;需要注意的是为了log计算中的underflow问题，可以对log运算进行转化&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/17.png&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;gradient&quot;&gt;Gradient&lt;/h1&gt;

&lt;p&gt;这里主要参考了&lt;a href=&quot;https://www.cs.toronto.edu/~graves/preprint.pdf&quot;&gt;Supervised Sequence Labelling with Recurrent
Neural Networks的第七章第四小节&lt;/a&gt;和&lt;a href=&quot;https://blog.csdn.net/xmdxcsj/article/details/51763886&quot;&gt;CTC学习笔记&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;其中第一份参考资料的式（7.32）应该是错了，多了一个负号。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/18.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/19.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/20.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;计算梯度的过程中，需要注意一点是，如果被求偏导的节点不在当前求偏导的路径上，那么他的倒数为0&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/CTC/21.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在学习CTC的时候，主要是在理解前后向算法，以及推导梯度公式。&lt;br /&gt;
因为它的代码实现比较困难，所以我没有直接运用上述内容进行编程练习，用的只是tensorflow的API，后面有时间的会找一些别人的实现看看，然后再试着自己去实现。&lt;/p&gt;

&lt;h1 id=&quot;cnn_lstm_ctc_hdr&quot;&gt;cnn_lstm_ctc_HDR&lt;/h1&gt;

&lt;p&gt;上星期另一个主要的任务是写了一个手写数字的识别。因为是第一次完全自己写，遇到的问题比较多，花了比较多的时间。&lt;/p&gt;

&lt;p&gt;目前还没有完成，我觉得还欠缺两个比较重要的东西。&lt;br /&gt;
一个是评判机制，因为label是不定长的，而且感觉不能以单纯对错来评估结果，比如说正确的是[1234],预测是[2234],所以还在找一个比较合理的准确率计算方式。&lt;br /&gt;
另一个是因为CTC出来的是一个概率，需要进行解码才能得出直观的结果，目前这一步还没有完成&lt;/p&gt;

&lt;p&gt;另外现在的网络存在过拟合问题，应该是数据集太小的缘故。&lt;/p&gt;

&lt;p&gt;采用的网络结构是cnn+lstm+ctc&lt;br /&gt;
cnn使用了三层卷积网络和两层全链接层，提取出特征作为lstm的输入。&lt;/p&gt;

&lt;p&gt;使用的数据集参考了&lt;br /&gt;
https://arxiv.org/pdf/1710.03112.pdf&lt;br /&gt;
目前只使用了cvl-strings，里面只有1262张图片。&lt;/p&gt;

&lt;p&gt;编程过程中需要注意的的地方有&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;label数目要计算上blank，即在原来classnum基础上 + 1&lt;/li&gt;
  &lt;li&gt;注意数据的格式，要符合tensorflow的要求&lt;/li&gt;
  &lt;li&gt;因为label不定长，需要将其转换成稀疏矩阵&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这是代码的&lt;a href=&quot;https://github.com/jhl13/cnn_lstm_ctc_HDR/blob/master/rnn.ipynb&quot;&gt;github仓库&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;参考资料：&lt;br /&gt;
http://ilovin.me/2017-04-23/tensorflow-lstm-ctc-input-output/&lt;br /&gt;
http://ilovin.me/2017-04-06/tensorflow-lstm-ctc-ocr/&lt;br /&gt;
https://github.com/xiaofengShi/CTC_TF/blob/padding_warpctc/lib/networks/network.py&lt;br /&gt;
https://github.com/jimmyleaf/ocr_tensorflow_cnn&lt;/p&gt;</content><author><name>luo_13</name></author><category term="ML" /><summary type="html">主要介绍CTC的概念</summary></entry></feed>