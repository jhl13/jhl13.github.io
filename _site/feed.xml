<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-11-20T12:07:29+08:00</updated><id>http://localhost:4000/</id><title type="html">luo_13的博客</title><subtitle>别看了，没啥好看的。</subtitle><author><name>luo_13</name></author><entry><title type="html">深度网络结构简介</title><link href="http://localhost:4000/ml/%E5%B8%B8%E7%94%A8%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/" rel="alternate" type="text/html" title="深度网络结构简介" /><published>2018-11-20T00:00:00+08:00</published><updated>2018-11-20T00:00:00+08:00</updated><id>http://localhost:4000/ml/%E5%B8%B8%E7%94%A8%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84</id><content type="html" xml:base="http://localhost:4000/ml/%E5%B8%B8%E7%94%A8%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/">&lt;h1 id=&quot;深度网络结构&quot;&gt;深度网络结构&lt;/h1&gt;
&lt;p&gt;深度网络结构的发展是相互影响的，新网络结构往往会受以往的网络结构影响，而新的网络结构也往往会对旧网络结构进行针对性的改进。这里主要介绍学习内容里面说到的六种深度网络框架，虽不涉及反向传播的推导过程，但是对我们理解这些网络框架会有挺大的帮助。&lt;/p&gt;

&lt;h2 id=&quot;alexnet&quot;&gt;AlexNet&lt;/h2&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/常用网络结构/1.bmp&quot; /&gt; 
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
AlexNet应该是正式拉开深度学习热潮的一个网络结构，相比于LeNet在网络深度上有了很大的提升。如上图所示，AlexNet包括了五层卷积层和三层全连接层，图中将网络整体分成两部分是因为当时的硬件计算能力并不支持这么大的网络，现在如果使用较好的GPU就不需要担心这个问题了，可以将其合并成一个部分。&lt;/p&gt;

&lt;p&gt;AlexNet在当时比较突出的地方是：1、使用了非线性激活函数：ReLU；2、为防止过拟合使用了Dropout和数据扩充；3、使用了LRN归一化。&lt;/p&gt;

&lt;p&gt;值得注意的是，在后面的VGGNet中指出了，LRN归一化其实并没有明显的作用。但是在Tensorflow中仍然保存有LRN归一化的API接口，这个后面可以再深入了解一下。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://my.oschina.net/u/876354/blog/1633143&quot;&gt;参考链接&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;vggnet&quot;&gt;VGGNet&lt;/h2&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/常用网络结构/2.bmp&quot; /&gt; 
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
这是VGGNet中实验过的网络结构，其中以VGG16和VGG19最为著名。VGGNet相比于AlexNet有着更深的网络结构，其次，VGGNet为了减少参数且增加模型的非线性化特性，使用了重叠的3X3卷积核去代替AlexNet中的5X5卷积核。（作者认为两个3x3的卷积堆叠获得的感受野大小，相当一个5X5的卷积；而3个3x3卷积的堆叠获取到的感受野相当于一个7x7的卷积）。池化核也从3X3变为2X2。还有一点是，VGGNet在测试阶段将全连接层化成了1X1的卷积层，使网络可以处理任意分辨率的图片。
需要注意的是，现在很多应用仍使用全连接层而不是1X1的卷积层，这说明1X1的卷积层其实并不能代替全连接层，这其中的原因可以去思考一下。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://my.oschina.net/u/876354/blog/1634322&quot;&gt;参考链接&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;googlenet&quot;&gt;GoogLeNet&lt;/h2&gt;

&lt;p&gt;GoogLeNet深度有22层，但大小却比AlexNet和VGG小很多，GoogleNet参数为500万个，AlexNet参数个数是GoogleNet的12倍，VGGNet参数又是AlexNet的3倍。究其原因是GoogLeNet对网络结构进行了大胆的尝试，将全连接改成稀疏链接，引入Inception网络结构，搭建一个稀疏性、高计算性能的网络结构。但是在实际应用上GoogLeNet在网络的最后还是加上了全连接层，比较著名的的GoogLeNet有四种V1、V2、V3、V4。为了避免梯度消失的情况，GoogLeNet采用了两个辅助分类器，以增加反向传播时的梯度。
&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/常用网络结构/3.bmp&quot; /&gt; 
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;V1的Inception网络结构加入1x1卷积核主要是为了减少通道维度，以减少参数量和计算量。&lt;/p&gt;

&lt;p&gt;V2和V3的Inception网络结构则为了进一步减少参数，都是着重进行了卷积分解，V2将5x5卷积核替换为3x3卷积核，V3将nxn的卷积核替换为1xn和nx1的卷积核。&lt;/p&gt;

&lt;p&gt;V4则结合了残差网络进一步加深网络结构。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://my.oschina.net/u/876354/blog/1637819&quot;&gt;参考链接&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;resnet&quot;&gt;ResNet&lt;/h2&gt;

&lt;p&gt;通过实验可以发现：随着网络层级的不断增加，模型精度不断得到提升，而当网络层级增加到一定的数目以后，训练精度和测试精度迅速下降，这说明当网络变得很深以后，深度网络就变得更加难以训练了。&lt;/p&gt;

&lt;p&gt;ResNet的由来：假设现有一个比较浅的网络（Shallow Net）已达到了饱和的准确率，这时在它后面再加上几个恒等映射层（Identity mapping，也即y=x，输出等于输入），这样就增加了网络的深度，并且起码误差不会增加，也即更深的网络不应该带来训练集上误差的上升。而这里提到的使用恒等映射直接将前一层输出传到后面的思想，便是著名深度残差网络ResNet的灵感来源。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/常用网络结构/4.bmp&quot; /&gt; 
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;当训练时模型精度不变甚至下降时，将学习目标从最小化loss改成学习F（x）=0，使H（x）=x，也就是变成了恒等映射的学习。这样可以在保持精度的情况下继续加深网络。需要注意的时，便捷链接可能链接到两个通道数量不一样的层，所以要使用H(x)=F(x)+Wx去调整x的维数。&lt;/p&gt;

&lt;p&gt;疑问：学习恒等映射应该只是保持精度不变，为什么实际上会对精度有提升作用。&lt;/p&gt;

&lt;h2 id=&quot;实验内容&quot;&gt;实验内容&lt;/h2&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/常用网络结构/5.bmp&quot; /&gt; 
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;基于Tensorflow对VGGNet进行finetune，重新训练后面三层全连接层参数，并在kaggle的猫狗训练集上进行实验，最后达到96.88%的正确率。发现一个比较有趣的事情，在二分类问题中如果对网络参数进行随机初始化，分类正确率基本在50%左右，应该与参数的随机初始化服从标准正态分布有关。&lt;/p&gt;</content><author><name>luo_13</name></author><category term="ML" /><summary type="html">介绍几种比较常用的深度网络结构</summary></entry><entry><title type="html">softmax &amp;amp; SVM</title><link href="http://localhost:4000/ml/softmax&SVM/" rel="alternate" type="text/html" title="softmax &amp; SVM" /><published>2018-10-14T00:00:00+08:00</published><updated>2018-10-14T00:00:00+08:00</updated><id>http://localhost:4000/ml/softmax&amp;SVM</id><content type="html" xml:base="http://localhost:4000/ml/softmax&amp;SVM/">&lt;h1 id=&quot;softmax--svm&quot;&gt;softmax &amp;amp; SVM&lt;/h1&gt;

&lt;p&gt;因为这几天接触得比较多关于SVM和softmax的内容，所以去复习了一下。&lt;/p&gt;

&lt;p&gt;softmax主要是重新了解了一下他的定义以及关于softmax的几种优化算法&lt;/p&gt;

&lt;h2 id=&quot;original-softmax&quot;&gt;original softmax&lt;/h2&gt;

&lt;p&gt;softmax可以看成是logistic回归模型在多分类上的推广&lt;/p&gt;

&lt;p&gt;以前学习softmax比较疑惑的一点是，他的loss function是怎么得出来的，这次复习的时候，从吴恩达机器学习关于logistic回归模型的cost函数找到了结果，softmax的损失函数是根据最大似然估计推导得到的，其实虽然说的是logistic回归模型，但是根据这个可以很容易推广导softmax的交叉熵损失函数当中。找了一篇&lt;a href=&quot;https://blog.csdn.net/ligang_csdn/article/details/53838743&quot;&gt;博客&lt;/a&gt;，具体的推导过程在3.2节。&lt;/p&gt;

&lt;p&gt;另外还重新看了一遍&lt;a href=&quot;http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92&quot;&gt;ufldl的softmax部分 &lt;/a&gt;
对softmax的参数冗余和权重衰减有了一些新的看法。&lt;/p&gt;

&lt;p&gt;softmax的参数冗余会造成采用牛顿法优化代价函数时遇到数值计算的问题，具体原因是Hessian矩阵不可逆，而加入权重衰减就是为了惩罚过大的参数值，使代价函数变成一个严格的凸函数，使Hessian矩阵可逆。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E5%87%B8%E5%87%BD%E6%95%B0&quot;&gt;凸函数的定义&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;严格凸函数的定义:&lt;/p&gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/softmax&amp;amp;SVM/1.png&quot; /&gt; 
&lt;/div&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/softmax&amp;amp;SVM/2.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;通过相似三角形的性质可以证明这一个结论&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/softmax&amp;amp;SVM/3.jpg&quot; /&gt; 
&lt;/div&gt;

&lt;h2 id=&quot;介绍两种softmax-loss的优化算法&quot;&gt;介绍两种softmax loss的优化算法&lt;/h2&gt;

&lt;p&gt;一个是L-softmax loss&lt;br /&gt;
&lt;a href=&quot;https://blog.csdn.net/u014380165/article/details/76864572&quot;&gt;Large-Margin softmax loss&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;另一个是A-softmax loss&lt;br /&gt;
&lt;a href=&quot;https://blog.csdn.net/u014380165/article/details/76946380&quot;&gt;angular softmax loss&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;还没有深入去推导过他们，但是大概知道是怎么一回事，做的事情是将映射空间（不知道这个说法合不合适）从概率转换导角度，从而使类间距离更大，类内距离更小。&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/softmax&amp;amp;SVM/4.jpg&quot; /&gt; 
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
A-softmax loss相比于L-softmax loss来说，多了两个条件&lt;br /&gt;
||W1||=||W2||=1以及b=0&lt;br /&gt;
使得类间、类内距离只取决与W和x的角度，而跟半径长度无关。&lt;/p&gt;

&lt;p&gt;其实我觉得师兄推荐的cosface对应论文提出LMCL也是对softmax的一种优化，只是是softmax条件更苛刻，使类间更远，类内更紧凑&lt;/p&gt;

&lt;p&gt;关于softmax的一个&lt;a href=&quot;https://zhuanlan.zhihu.com/p/34044634&quot;&gt;综述&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;关于SVM主要是从新看了一下各种内核的适用情况，资料是吴恩达coursera上的机器学习课程。&lt;/p&gt;

&lt;p&gt;其中比较常用的是linear kernel以及gaussian kernel&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;linear kernel用在特征很多，但是训练样本较少的情况。&lt;/li&gt;
  &lt;li&gt;guassian kernel用在特征很少，训练样本适中的情况。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果特征很少，样本很多，应该先使用方法去增加特征数，然后使用线性内核的SVM或直接使用多个logistic regression回归函数。&lt;/p&gt;</content><author><name>luo_13</name></author><category term="ML" /><summary type="html">主要介绍softmax &amp; SVM</summary></entry><entry><title type="html">MTCNN &amp;amp; FaceNet</title><link href="http://localhost:4000/ml/MTCNN&FaceNet/" rel="alternate" type="text/html" title="MTCNN &amp; FaceNet" /><published>2018-10-03T00:00:00+08:00</published><updated>2018-10-03T00:00:00+08:00</updated><id>http://localhost:4000/ml/MTCNN&amp;FaceNet</id><content type="html" xml:base="http://localhost:4000/ml/MTCNN&amp;FaceNet/">&lt;p&gt;#简介
这里主要是对两篇论文的理解&lt;br /&gt;
1、《Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks》&lt;br /&gt;
2、《FaceNet: A Unified Embedding for Face Recognition and Clustering》&lt;/p&gt;

&lt;h2 id=&quot;mtcnn&quot;&gt;MTCNN&lt;/h2&gt;

&lt;p&gt;mtcnn同时完成了人脸检测（recognition）以及人脸对齐（alignment）的任务。&lt;/p&gt;

&lt;p&gt;创新的地方:&lt;br /&gt;
1.  使用了三层联级结构&lt;br /&gt;
2. 考虑到face recognition和alignment的固有联系同时进行face recognition和alignment&lt;br /&gt;
3. 提供了一种名为Online Hard sample mining的训练方法，且在实际中效果不错&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;mtcnn分为三层，proposal net（P-Net）、refinement net（R-Net）、output net（O-Net）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/MTCNN&amp;amp;FaceNet/1.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;P-Net用于生成候选窗口，R-Net用于去除错误的候选窗口，O-Net的效果与R-Net相识，但条件更加严格，用于最后输出bounding box以及facial landmarks&lt;br /&gt;
在P-Net和R-Net中会用NMS（non-maximum suppression）的方式去合并高重合度的候选窗口&lt;/p&gt;

&lt;p&gt;NMS（non-maximum suppression）是一个迭代-遍历-消除的过程&lt;br /&gt;
（1）将所有框的得分排序，选中最高分及其对应的框&lt;br /&gt;
（2）遍历其余的框，如果和当前最高分框的重叠面积(IOU)大于一定阈值，我们就将框删除&lt;br /&gt;
（3）从未处理的框中继续选一个得分最高的，重复上述过程。&lt;br /&gt;
&lt;a href=&quot;https://blog.csdn.net/shuzfan/article/details/52711706&quot;&gt;参考资料&lt;/a&gt;
还没去看源代码，但是感觉应该不会只是单纯的删除候选窗口，以前有看过使用合并的。&lt;/p&gt;

&lt;h3 id=&quot;联级网络结构&quot;&gt;联级网络结构&lt;/h3&gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/MTCNN&amp;amp;FaceNet/2.png&quot; /&gt; 
&lt;/div&gt;

&lt;h3 id=&quot;网络结构&quot;&gt;网络结构&lt;/h3&gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/MTCNN&amp;amp;FaceNet/3.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;作者采用PRelu作为conv和fc层的激活函数，PRelu与Relu的区别在于在负半轴PRelu不饱和，对于这两个激活函数应该没有优劣的说法，但是有&lt;a href=&quot;https://arxiv.org/pdf/1502.01852.pdf&quot;&gt;论文&lt;/a&gt;表明使用PRelu在分类问题上会有一点提升&lt;/p&gt;
&lt;h3 id=&quot;训练过程&quot;&gt;训练过程&lt;/h3&gt;
&lt;p&gt;分成了三个部分&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;face/non-face classification&lt;br /&gt;
bounding box regression&lt;br /&gt;
facial landmark localization&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;face/non-face classification使用cross-entropy loss&lt;br /&gt;
bounding box regression, facial landmark localization使用Euclidean loss&lt;br /&gt;
文章中还提到了Multi-source training，其中要注意的是训练过程中因为使用了不同的训练图片，比如有些只是标注了是否是脸部，这种情况就只训练face/non-face &lt;br /&gt;
classification而不训练bounding box regression, facial landmark localization&lt;br /&gt;
Online Hard sample mining：在前向传播中使用全部样本，而反向传播只使用了其中的70%样本&lt;/p&gt;

&lt;p&gt;在训练过程中使用了四种不同的数据注释：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Negatives: IoU &amp;lt; 0.3&lt;br /&gt;
positives: IoU &amp;gt; 0.65&lt;br /&gt;
Part faces:   0.4&amp;lt;IoU&amp;lt;0.65&lt;br /&gt;
Landmark faces: landmarks五点轮廓&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这四个分类分别用来训练不同的部分，这一部分的意思还不是很懂，后面结合代码再看看。&lt;/p&gt;

&lt;h2 id=&quot;facenet&quot;&gt;FaceNet&lt;/h2&gt;

&lt;p&gt;facenet作者提供了caffe的源代码，而且github上有使用tensorflow的实现。&lt;br /&gt;
facenet可以用于人脸验证face verification、人脸识别face recognition、以及聚类face clustering&lt;/p&gt;

&lt;p&gt;facenet 通过深度学习的方式将一幅图像映射到欧氏空间里，之后face verification就变成了评估距离的问题，face recognition就成了KNN分类问题，face clustering就相当与K-means问题&lt;/p&gt;

&lt;p&gt;创新的地方：&lt;br /&gt;
1、减少输出维度至128维，减少了计算量&lt;br /&gt;
2、使用基于LMNN的triplet-based loss function&lt;br /&gt;
&lt;a href=&quot;http://www.jmlr.org/papers/volume10/weinberger09a/weinberger09a.pdf&quot;&gt;LMNN&lt;/a&gt;类似与SVM，但其中用knn代替了线性分类&lt;/p&gt;

&lt;p&gt;论文使用了两种深层卷积网络结构&lt;br /&gt;
the Zeiler&amp;amp;Fergus model以及the Inception model of Szegedy，其中在the Zeiler&amp;amp;Fergus model的卷积层之间加入1x1xd的卷积层。关于1x1xd的卷积层，作者说是受这篇&lt;a href=&quot;https://arxiv.org/pdf/1312.4400.pdf&quot;&gt;文章&lt;/a&gt;影响，后面再看。FLPS为每秒计算浮点数的次数。&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/MTCNN&amp;amp;FaceNet/4.png&quot; /&gt; 
&lt;/div&gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/MTCNN&amp;amp;FaceNet/5.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;triplets包括两个匹配的面部缩略图，一个错误匹配的面部缩略图&lt;br /&gt;
triplet loss用于最小化目标与正确匹配的缩略图之间的距离，最大化与错误匹配的缩略图之间的距离&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/MTCNN&amp;amp;FaceNet/6.png&quot; /&gt; 
&lt;/div&gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/MTCNN&amp;amp;FaceNet/7.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;由（1）可以可得，triplets需要计算argmax和argmin。&lt;br /&gt;
生成triplets的方式有两种，每个step生成一次，每个mini-batch生成一次。对于文章里说到的两种方式还是有点模糊，往后再通过代码加深理解 &lt;br /&gt;
文章采用第二种方式，同时为了避免负样本选择不当而导致训练陷入局部最优解，将计算公式更改为&lt;/p&gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/MTCNN&amp;amp;FaceNet/8.png&quot; /&gt; 
&lt;/div&gt;

&lt;h3 id=&quot;模型结构&quot;&gt;模型结构&lt;/h3&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/MTCNN&amp;amp;FaceNet/9.png&quot; /&gt; 
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
&lt;b&gt;目前不是很懂的地方：&lt;/b&gt;face embedding代表的是什么？&lt;/p&gt;

&lt;p&gt;代码实现：&lt;br /&gt;
使用了预训练好的模型，现在只是跑通了程序&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/MTCNN&amp;amp;FaceNet/10.png&quot; /&gt; 
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
用的是第一个模型&lt;br /&gt;
遇到的问题，模型比较大，电脑的GPU内存（2G）比较小，要将利用率提高到90%才能正常运行。&lt;br /&gt;
tensorflow版本不同，出现了一些warning，后续可能需要修改一下。&lt;br /&gt;
测试LFW结果与预期结果大致一样&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/MTCNN&amp;amp;FaceNet/11.png&quot; /&gt; 
&lt;/div&gt;</content><author><name>luo_13</name></author><category term="ML" /><summary type="html">介绍人脸检测模型MTCNN以及人脸识别模型FaceNet</summary></entry><entry><title type="html">递归全排列</title><link href="http://localhost:4000/c++%20language/%E9%80%92%E5%BD%92%E5%85%A8%E6%8E%92%E5%88%97/" rel="alternate" type="text/html" title="递归全排列" /><published>2018-09-22T00:00:00+08:00</published><updated>2018-09-22T00:00:00+08:00</updated><id>http://localhost:4000/c++%20language/%E9%80%92%E5%BD%92%E5%85%A8%E6%8E%92%E5%88%97</id><content type="html" xml:base="http://localhost:4000/c++%20language/%E9%80%92%E5%BD%92%E5%85%A8%E6%8E%92%E5%88%97/">&lt;h1 id=&quot;递归全排列&quot;&gt;递归全排列&lt;/h1&gt;

&lt;p&gt;先看代码&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Perm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; 
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; 
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;// 从固定的数后第一个依次交换
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;Swap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;Perm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;// 这组递归完成之后需要交换回来
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;Swap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Swap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;递归全排列的思路&quot;&gt;递归全排列的思路&lt;/h2&gt;

&lt;p&gt;拿list={1234}做一个例子&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;list(0)与list(0)交换 -&amp;gt; list{1234} -&amp;gt; 固定第一位得到 1+list{234}&lt;/li&gt;
  &lt;li&gt;list(1)与list(1)交换 -&amp;gt; list{1234} -&amp;gt; 固定一二位得到 12+list{34}&lt;/li&gt;
  &lt;li&gt;list(2)与list(2)交换 -&amp;gt; list{1234} -&amp;gt; 固定一二三位得到 123+list{4}&lt;/li&gt;
  &lt;li&gt;已是最后一位，输出 1234&lt;/li&gt;
  &lt;li&gt;将list(2)与list(2)复位 -&amp;gt; list{1234}&lt;/li&gt;
  &lt;li&gt;list(2)与list(3)交换 -&amp;gt; list{1243} -&amp;gt; 固定一二三位得到 124+list{3}&lt;/li&gt;
  &lt;li&gt;已是最后一位，输出 1243&lt;/li&gt;
  &lt;li&gt;将list(2)与list(3)复位 -&amp;gt; list{1234}&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;将list(1)与list(1)复位 -&amp;gt; list{1234}&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;list(1)与list(2)交换 -&amp;gt; list{1324} -&amp;gt; 固定一二位得到 13+list{24}&lt;/li&gt;
  &lt;li&gt;list(2)与list(2)交换 -&amp;gt; list{1324} -&amp;gt; 固定一二三位得到 132+list{4}&lt;/li&gt;
  &lt;li&gt;已是最后一位，输出 1324&lt;/li&gt;
  &lt;li&gt;将list(2)与list(2)复位 -&amp;gt; list{1324}&lt;/li&gt;
  &lt;li&gt;list(2)与list(3)交换 -&amp;gt; list{1342} -&amp;gt; 固定一二三位得到 134+list{2}&lt;/li&gt;
  &lt;li&gt;已是最后一位，输出 1342&lt;/li&gt;
  &lt;li&gt;将list(2)与list(3)复位 -&amp;gt; list{1324}&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;将list(1)与list(2)复位 -&amp;gt; list{1234}&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;list(1)与list(3)交换 -&amp;gt; list{1432} -&amp;gt; 固定一二位得到 14+list{32}&lt;/li&gt;
  &lt;li&gt;list(2)与list(2)交换 -&amp;gt; list{1432} -&amp;gt; 固定一二三位得到 143+list{2}&lt;/li&gt;
  &lt;li&gt;已是最后一位，输出 1432&lt;/li&gt;
  &lt;li&gt;将list(2)与list(2)复位 -&amp;gt; list{1432}&lt;/li&gt;
  &lt;li&gt;list(2)与list(3)交换 -&amp;gt; list{1423} -&amp;gt; 固定一二三位得到 142+list{3}&lt;/li&gt;
  &lt;li&gt;已是最后一位，输出 1423&lt;/li&gt;
  &lt;li&gt;将list(2)与list(3)复位 -&amp;gt; list{1432}&lt;/li&gt;
  &lt;li&gt;将list(1)与list(3)复位 -&amp;gt; list{1234}&lt;/li&gt;
  &lt;li&gt;将list(0)与list(0)复位 -&amp;gt; list{1234}&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;上面列出了固定第一个数 {1} 时的全排列，后面的{2}、{3}、{4}可以以此类推&lt;/p&gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/递归全排列/1.png&quot; /&gt; 
&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/zyoung/p/6764371.html&quot;&gt;图片来源&lt;/a&gt;&lt;/p&gt;</content><author><name>luo_13</name></author><category term="C++ language" /><summary type="html">利用递归函数实现全排列</summary></entry><entry><title type="html">简介-模板与重载</title><link href="http://localhost:4000/c++%20language/%E6%A8%A1%E6%9D%BF%E4%B8%8E%E9%87%8D%E8%BD%BD/" rel="alternate" type="text/html" title="简介-模板与重载" /><published>2018-09-15T00:00:00+08:00</published><updated>2018-09-15T00:00:00+08:00</updated><id>http://localhost:4000/c++%20language/%E6%A8%A1%E6%9D%BF%E4%B8%8E%E9%87%8D%E8%BD%BD</id><content type="html" xml:base="http://localhost:4000/c++%20language/%E6%A8%A1%E6%9D%BF%E4%B8%8E%E9%87%8D%E8%BD%BD/">&lt;h1 id=&quot;c模板函数与重载函数&quot;&gt;C++模板函数与重载函数&lt;/h1&gt;

&lt;h2 id=&quot;模板函数&quot;&gt;模板函数&lt;/h2&gt;

&lt;p&gt;可以理解为一段通用代码&lt;br /&gt;
形参类型由编译器决定&lt;br /&gt;
下述类型要相同，不同编译器会报错&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;更通用的版本&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Ta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Ta&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Ta&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tb&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tc&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;重载函数&quot;&gt;重载函数&lt;/h2&gt;

&lt;p&gt;定义多个同名函数的机制称为函数重载&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;
&lt;p&gt;模板函数和重载函数有什么不同&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;模板函数只能定义一个&lt;/li&gt;
  &lt;li&gt;重载函数可以多个&lt;/li&gt;
  &lt;li&gt;模板函数参数数量固定，而重载函数可以不定&lt;/li&gt;
&lt;/ul&gt;</content><author><name>luo_13</name></author><category term="C++ language" /><summary type="html">模板函数与重载函数</summary></entry><entry><title type="html">简介-RNN</title><link href="http://localhost:4000/ml/%E7%AE%80%E4%BB%8B-RNN/" rel="alternate" type="text/html" title="简介-RNN" /><published>2018-09-15T00:00:00+08:00</published><updated>2018-09-15T00:00:00+08:00</updated><id>http://localhost:4000/ml/%E7%AE%80%E4%BB%8B-RNN</id><content type="html" xml:base="http://localhost:4000/ml/%E7%AE%80%E4%BB%8B-RNN/">&lt;h1 id=&quot;vanilla-rnn&quot;&gt;vanilla RNN&lt;/h1&gt;

&lt;hr /&gt;

&lt;p&gt;RNN最大的特点在于同一隐藏层的各单元按照序列链接。&lt;/p&gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/RNN/1.png&quot; /&gt; 
&lt;/div&gt;
&lt;p&gt;通过这个图可以知道，RNN比CNN更适合处理具有一定序列的输入，因为RNN的结构决定了前面的输入必然能影响到后面的输出。这使得RNN在NLP这一领域获得了很大的成功。&lt;/p&gt;

&lt;p&gt;在学习RNN的过程过程中反向传播公式的推导应该是比较难的一个部分，因为像是CNN的反向传播只有一个方向，就是上一层传递给下一层，而RNN除了沿上下层的传播还有沿时间序列方向的传播。&lt;/p&gt;

&lt;p&gt;最终推导结果：&lt;br /&gt;
c为隐藏层到输出的bias，b为输入到隐藏层的bais，推导过程比较长，以下这篇文章写得比较清晰&lt;a href=&quot;https://zybuluo.com/hanbingtao/note/541458&quot;&gt;循环神经网络&lt;/a&gt;以及&lt;a href=&quot;http://www.deeplearningbook.org/contents/rnn.html&quot;&gt;deeplearningbook&lt;/a&gt;写得比较清晰&lt;/p&gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/RNN/2.png&quot; /&gt; 
&lt;/div&gt;
&lt;p&gt;在编程过程中发现了一个比较容易出错的地方，那就是反向传播过程要注意残差在上下层之间传递，因为参考内容的推导过程主要是以一个隐藏层作为例子，所以写代码的过程要注意这一点，如下图，残差3要往4和5的方向传递，但要注意的是，残差3是有1,2两个方向的残差累加而成的。&lt;/p&gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/RNN/3.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;另一个需要注意的地方是，在某些任务（不是全部）中RNN的训练和测试是很不一样的，这里举的例子是cs231n的image captioning 问题。&lt;/p&gt;

&lt;p&gt;训练时因为知道正确的label，所以可以将ground-truth直接当成input输入到RNN当中，但是训练的时候，因为不知道ground-truth，所以要将前一个输出要当成后一个的输入。&lt;br /&gt;
拿上面的图为例就是用x(t – 1)预测出o(t-1),然后将o(t-1)当成x(t)再次输入到网络。&lt;/p&gt;

&lt;p&gt;采用这种方式是因为cs231n的assignment3将图像的特征当成input输入到网络时只会预测出结果中的第一个词。&lt;br /&gt;
采用的结构感觉是在one to many上做了一些变动。&lt;/p&gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/RNN/4.png&quot; /&gt; 
&lt;/div&gt;

&lt;h2 id=&quot;vanilla-rnn的缺点&quot;&gt;Vanilla RNN的缺点&lt;/h2&gt;
&lt;p&gt;训练过程中容易发生梯度爆炸或梯度消失，产生的原因在参考资料&lt;a href=&quot;https://zybuluo.com/hanbingtao/note/541458&quot;&gt;循环神经网络&lt;/a&gt;中有详细说明。&lt;/p&gt;

&lt;p&gt;梯度爆炸可以简单用阈值方式解决。&lt;br /&gt;
而解决梯度消失也有挺多的方式，参考资料&lt;a href=&quot;https://zybuluo.com/hanbingtao/note/541458&quot;&gt;循环神经网络&lt;/a&gt;中提到了三种常见的方式，我觉得进行batch normalization也应该可以缓解这种现象。&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/RNN/5.png&quot; /&gt; 
&lt;/div&gt;

&lt;h1 id=&quot;lstm&quot;&gt;LSTM&lt;/h1&gt;

&lt;hr /&gt;

&lt;p&gt;如sigmoid函数，如果输入的模过大就会出现梯度消失的情况，LSTM为了解决这一问题加入了门和单元状态的概念，可以有效的控制前面输入对后面输出的影响。&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/RNN/6.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;LSTM添加了三个门，遗忘门（forget gate）用以决定上一时刻的单元状态有多少能保留下来，输入门（input gate）用来决定输入状态有多少能进入到单元状态，输出门（output gate）用来决定单元状态有多少能输出到output&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/RNN/7.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;c为单元状态（相当于一个容器），从左往右的三个X分别为遗忘门，输入门和输出门。&lt;br /&gt;
反向传播公式的推导在参考资料&lt;a href=&quot;http://www.deeplearningbook.org/contents/rnn.html&quot;&gt;deeplearningbook&lt;/a&gt;和&lt;a href=&quot;https://zybuluo.com/hanbingtao/note/581764&quot;&gt;LSTM&lt;/a&gt;中有很详细的过程&lt;/p&gt;

&lt;figure class=&quot;third&quot;&gt;
    &lt;img src=&quot;/images/RNN/8-1.png&quot; /&gt;
    &lt;img src=&quot;/images/RNN/8-2.png&quot; /&gt;
    &lt;img src=&quot;/images/RNN/8-3.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;状态单元c并不直接输出，在反向传播计算残差的时候要格外注意这一点。&lt;/p&gt;

&lt;p&gt;LSTM还有很多不同的变体，比如说GRU，GRU网上资料说是LSTM最为成功的一种变体，在后面的学习中会详细的去了解。&lt;/p&gt;

&lt;p&gt;一些想法&lt;br /&gt;
RNN比较适合处理时序问题，之前还看过一篇博客，内容主要是在介绍RNN能做一些什么事情，里面比较有趣的是让电脑学会从左往右去看一张图片。&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/RNN/9.gif&quot; /&gt; 
&lt;/div&gt;

&lt;h1 id=&quot;ocr&quot;&gt;OCR&lt;/h1&gt;

&lt;hr /&gt;

&lt;p&gt;对于text recognition有两种比较常见的方法&lt;br /&gt;
非端到端的OCR：需要进行预分割，特征提取，语义分析等环节，各环节之间存在一定关系&lt;br /&gt;
端到端OCR：无需进行预分割，且整个过程是一个整体&lt;/p&gt;</content><author><name>luo_13</name></author><category term="ML" /><summary type="html">主要介绍vanilla RNN与LSTM</summary></entry><entry><title type="html">简介-CTC</title><link href="http://localhost:4000/ml/%E7%AE%80%E4%BB%8B-CTC/" rel="alternate" type="text/html" title="简介-CTC" /><published>2018-09-15T00:00:00+08:00</published><updated>2018-09-15T00:00:00+08:00</updated><id>http://localhost:4000/ml/%E7%AE%80%E4%BB%8B-CTC</id><content type="html" xml:base="http://localhost:4000/ml/%E7%AE%80%E4%BB%8B-CTC/">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;h1 id=&quot;ctc&quot;&gt;CTC&lt;/h1&gt;

&lt;p&gt;CTC (Connectionist Temporal Classification) 是针对时间分类任务所设计出来的，利用它可以实现端到端的文本识别，在一定程度上解决了文本识别中的分割问题。&lt;/p&gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/CTC/1.png&quot; /&gt; 
&lt;/div&gt;
&lt;p&gt;通过上面的图片可以对CTC的工作有一个比较直观的理解。它不需要输入输出在每一个时刻都对齐，只要求最后的输出跟目标输出一致。&lt;/p&gt;

&lt;p&gt;CTC与别的算法主要区别在于，他要求输出序列不短于目标序列，且引入了Blank状态。&lt;/p&gt;

&lt;p&gt;如：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(a-ab-)=F(-aa--abb)=aab&lt;/script&gt;

&lt;p&gt;CTC主要在做的东西是预测每一个时刻的输出，然后合并相邻的重复输出，再移去blank状态。&lt;/p&gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/CTC/2.png&quot; /&gt; 
&lt;/div&gt;
&lt;p&gt;这里，两个L之间blank状态有着很关键的作用，它可以分隔开来两个相同的输出，避免了得到helo的结果。&lt;/p&gt;

&lt;p&gt;从上图可以看出，CTC对输出序列有两个要求，第一个是输入序列要不短于label长度;第二个是为了能区分开两个相同字母相邻的情况，需要对label做一下处理，通常的操作是在label中每个元素的前后加上blank状态，这样做会使label长度由U变成2U+1。&lt;/p&gt;

&lt;p&gt;如:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z=[ε,y1,εy2,...,ε,yu,ε]&lt;/script&gt;

&lt;h1 id=&quot;loss-function&quot;&gt;LOSS function&lt;/h1&gt;

&lt;p&gt;计算前向通道的时候，我们可以采用合并相同路线的方式去简化计算，也就是说每一个节点的概率只计算一次，这样可以减少很多计算量。这类似于HMM的forward-backword Algorithm。&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/CTC/3.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;计算前向通道的时候有一个值得注意的地方，那就是节点链接的合理性&lt;br /&gt;
比如说label集合是[ε1 a ε2 p ε3 p ε4 l ε5 e ε6],ε是blank，为了方便区分加了下  标，假设一个输出序列是正确的话&lt;br /&gt;
如果t时刻预测结果是e，则t-1的预测则一定是[l ε5 e]之间的一个;&lt;br /&gt;
如果 t时刻预测结果是ε6，则t-1的预测则一定是[e ε6]之间的一个;&lt;br /&gt;
如果 t时刻预测结果是p，则t-1的预测则一定是[ε3 p]之间的一个;&lt;br /&gt;
且一条正确的路径只能由[ε1 a]开始，以[e ε6]结束。&lt;/p&gt;

&lt;p&gt;所以前后节点的链接方式如下&lt;/p&gt;

&lt;center class=&quot;half&quot;&gt;
    &lt;img src=&quot;/images/CTC/4.png&quot; /&gt;  &lt;img src=&quot;/images/CTC/5.png&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;下图是一个有效路径的示意图&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/CTC/6.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;所以前后向算法在 &lt;br /&gt;
前向推导：&lt;br /&gt;
(1)初始化:&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/CTC/7.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;(2)递推关系：&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/CTC/8.png&quot; /&gt; 
&lt;/div&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/CTC/9.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;因为时间关系，有些路径不能到达终点 U‘-u-1 &amp;gt; 2(T-t)&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/CTC/10.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;同理后向推导：&lt;br /&gt;
(1)初始化:&lt;/p&gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/CTC/11.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;(2)递推关系：&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/CTC/12.png&quot; /&gt; 
  
   
&lt;img src=&quot;/images/CTC/13.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;CTC loss function使用的是最大似然&lt;/p&gt;

&lt;p&gt;取log之后&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/CTC/14.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;由前后向算法可得&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/CTC/15.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;所以&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/CTC/16.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;需要注意的是为了log计算中的underflow问题，可以对log运算进行转化&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/CTC/17.png&quot; /&gt; 
&lt;/div&gt;

&lt;h1 id=&quot;gradient&quot;&gt;Gradient&lt;/h1&gt;

&lt;p&gt;这里主要参考了&lt;a href=&quot;https://www.cs.toronto.edu/~graves/preprint.pdf&quot;&gt;Supervised Sequence Labelling with Recurrent
Neural Networks的第七章第四小节&lt;/a&gt;和&lt;a href=&quot;https://blog.csdn.net/xmdxcsj/article/details/51763886&quot;&gt;CTC学习笔记&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;其中第一份参考资料的式（7.32）应该是错了，多了一个负号。&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/CTC/18.png&quot; /&gt; 
  
   
&lt;img src=&quot;/images/CTC/19.png&quot; /&gt; 
  
   
&lt;img src=&quot;/images/CTC/20.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;计算梯度的过程中，需要注意一点是，如果被求偏导的节点不在当前求偏导的路径上，那么他的倒数为0&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;/images/CTC/21.png&quot; /&gt; 
&lt;/div&gt;

&lt;p&gt;在学习CTC的时候，主要是在理解前后向算法，以及推导梯度公式。&lt;br /&gt;
因为它的代码实现比较困难，所以我没有直接运用上述内容进行编程练习，用的只是tensorflow的API，后面有时间的会找一些别人的实现看看，然后再试着自己去实现。&lt;/p&gt;

&lt;h1 id=&quot;cnn_lstm_ctc_hdr&quot;&gt;cnn_lstm_ctc_HDR&lt;/h1&gt;

&lt;p&gt;这里写了一个手写数字的识别。问题还比较多，后面会继续完善。&lt;/p&gt;

&lt;p&gt;采用的网络结构是cnn+lstm+ctc&lt;br /&gt;
cnn使用了三层卷积网络和两层全链接层，提取出特征作为lstm的输入。&lt;/p&gt;

&lt;p&gt;使用的数据集参考了&lt;br /&gt;
https://arxiv.org/pdf/1710.03112.pdf&lt;br /&gt;
目前只使用了cvl-strings，里面只有1262张图片。&lt;/p&gt;

&lt;p&gt;编程过程中需要注意的的地方有&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;label数目要计算上blank，即在原来classnum基础上 + 1&lt;/li&gt;
  &lt;li&gt;注意数据的格式，要符合tensorflow的要求&lt;/li&gt;
  &lt;li&gt;因为label不定长，需要将其转换成稀疏矩阵&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这是代码的&lt;a href=&quot;https://github.com/jhl13/cnn_lstm_ctc_HDR/blob/master/rnn.ipynb&quot;&gt;github仓库&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;参考资料：&lt;br /&gt;
http://ilovin.me/2017-04-23/tensorflow-lstm-ctc-input-output/&lt;br /&gt;
http://ilovin.me/2017-04-06/tensorflow-lstm-ctc-ocr/&lt;br /&gt;
https://github.com/xiaofengShi/CTC_TF/blob/padding_warpctc/lib/networks/network.py&lt;br /&gt;
https://github.com/jimmyleaf/ocr_tensorflow_cnn&lt;/p&gt;</content><author><name>luo_13</name></author><category term="ML" /><summary type="html">主要介绍CTC的概念</summary></entry></feed>